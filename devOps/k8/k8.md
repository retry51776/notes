# Kubernetes
> Here is k8 architecture stuff.
>
> start from Google Borg, since 2014, under Cloud Native Computing Foundation (CNCF)
> 
> Infracture as code
>
> (K8 package)[https://artifacthub.io/]
> 
> k8 needs an `external loadbalancer`, or use MetaLB
>
> define Secret before RoleBind, Service before Deployment


## Things Cloud Providers make easier
- IAM `cloud provider already have buildin IAM w nice interface, self host k8 needs self manage auth by cert or service_account_token`
- External Loadbalancer for K8 `cloud have custom loadbalancer assign public ip, alt is MetalLB`
- Logging `all clouds have buildin logging, so many logs alternative but more work to setup, Kibana ELK?`
- Fancy UI way better than any extension
- Better Ingress `At least GKE's traefik is better`


## Analogy
> https://docs.google.com/presentation/d/1nOuYvXbQCjM_c869Mfyv59xfz-2f_QE4K1d0dUQQc4k/edit?usp=sharing

> **External Loadbalancer** (Public Street Entry)
>
> **namespace** (Section of Mall) `Group for k8s resources, allow Role to grant premission`
> 
> **network police** (Elevator) `One/Both way traffic control, help secure section of mall.`
> 
> **Cluster** (Shopping Mall) `Shopping Mall without parking lot, customers needs to walk into store`
> 
> **Node** (Building) `Shopping Mall can have only 1 building, or multi building connected together`
> 
> **Pods** (Area inside Shopping Mall Building) `can be [store/app deployment, hallway/loadbalancer, service area/job], Mall mgr able to scale multiple, or move store between building; Area by default is boarded up, to access must create door)`
> 
>> **Container** (SubArea inside Area) `Pod can have multi containers running, but they shares same area/ram, electricity/cpu`
>>
>> **Probs** (Construction Signs) `indicator Area is still remodeling or ready`

>> **Daemonset** (Building requirements) `Ex: every building should basic utilities: emergency exit, information desk, hallway, bathrooms. Similar Node requires logging, traffic control, metric, disk mount`
>
>> **Deployment** (Chain Stores) `do/sell same thing, coperate usually keep at least couple in each malll. scale dependence on traffic, allow multi stores in same building, or none in any building.`
>
>> **Naked Pod** (Mom & Pop Store) `do its own thing, none to restart when it closed`


> **Api Server** (Mall mgr)
>> **etcd** Mall mgr's notebook
>
>> **Controller, Scheduler** (Assistant of Mall mgr) `Controller decide, maintain service, auth; Scheduler check probs, where pods`
>
>> **kubelet** (Building mgr) `Listen to Mall mgr direction`
>
>> **kubeproxy** (Information Desk) `part of guide system`

> **Service** (Map of Mall) `part of guide system, 3 types(LoadBalancer, NodePort, ClusterIP)`
> 
> Endpoint (Store Number) `Property of Service; each store have its own unqiue number, Ex: Subway have 3 stores in Mall [x.x.x.1:3000, x.x.x.2:3000, x.x.x.3:3000]`
> 
>> 1. **LoadBalancer Service** (Entry Point) `Where customer go into mall, able to handle more traffic, in cost of expensive construction`
>>> **Ingress Service** (Hallway Entry) 
>>>> **Ingress Controller** (Hallway) `The place where we direct customers to stores(ClusterIP Service), recommend to be daemonset(similar to each building should have its own hallway)`
>>>>> **Ingress Rule** (Hallway Direction) `How we route customers to Stores; layer 7 Application & layer 4 Transport`
>>
>>> **LoadBalancer Service** (Coner Store Entry) `Coner Store(Ex: public UI, API) expects a lot traffic will needs its own door(public access)`
>> 
>> 2. **NodePort Service** (Service Door of Store in Shopping Mall) `allow developer testing, usually not use in production`
>> 
>> 3. **ClusterIP Service** (Store Door inside Shopping Mall) `have smaller limited traffic support`



# Workflow:
1. Authentication     [cert, token]
2. Authorization      [RBAC, verb, resource, apigroup, namespace]
3. Mutating webhook   (opinional)
4. AdmissionReview    
5. Validating webhook (opinional)
6. Lock etcd          (only on edit)
7. Edit etcd          (only on edit)
8. api_service watch trigger kube-scheduler 
9. kube-scheduler assign new Pod on some Node w enough resources
10. kubelet send Container Runtime Interface (CRI) request
11. Container Runtime Interface (CRI) calls Container Network Interface(CNI), assign IP for pod
12. Container Network Interface(CNI) call Container Runtime create POD

## Objects / Resources / Manifests
> resources types: https://kubernetes.io/docs/reference/kubectl/overview/#resource-types
> 
> custom resource definition (CRD) since k8 1.7
>
> annotations is NOT used by selector, selector = label
>
> apiversions value dependenced on Kind, check here:
>
> https://matthewpalmer.net/kubernetes-app-developer/articles/kubernetes-apiversion-definition-guide.html
>
> https://kubernetes.io/docs/concepts/overview/working-with-objects/

> Examples of controllers are Deployments, ReplicaSets, DaemonSets, Jobs, etc.
>
> multi control loop & operator(custom control loop, Ex: MySQL Operator)
>
- config `store contexts, where cluster, who is user, ca,`
- cronjob `https://crontab.guru/`
- daemonset `similar to deployment, but every node have 1; good for logging, ingress controller,`
- deployment `Add deployment_strategy, Pros,... on top ReplicaSet, `
- ReplicaSet (RS) `a very basic unit k8 start pods`
- pods `smalleset k8 unit`
- service `exposed to other k8 services, append records into service registry`
- ingress `exposed to vpn`
- event `api_service only store 1hr events`
- secrets
- configMap
- persistentVolume(pv) `where folder is`
- persistentVolumeClaim (pvc) `mount to pod, lock system avoid multi pod mount same pv`
- ResourceQuota `apply to namespace to limit CPU & RAM`

## DaemonSets
> enforce single pod per node on all nodes (for monitor logs, storage, network)
```yml
nodeSelector:
  name: xxx_node_name
tolerations:
- key: node.kubernetes.io/unschedulable
  operator: Exists
  effect: NoSchedule   # No new pod until daemonset ready
```

## Ingress
> ingress is K8 route controller, by default is nginx, alt is nginx+(Dashboard, LB methods, sess persistance, health check, JWT validation), or traefik, HAProxy https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/

### Ingress Types:
- Singel Service
- Simple Fanout
- Name-based virtual hosting
  
## K8 components:
- Mater Node
  - Scheduler
    - Trigger Cron Jobs
  - Control Mgr
    - Compare Active State vs Desired State
  - etcd
    - key/value DB
  - API service
    - Supports cmds
- Worker Node
  - Container Run Time
    - Pods //smallest k8 unit
  - Kubproxy
    - route & selector
  - Kubelet
    - communicate with API-service

## Node Components
- kubelet
  1. Scheduled: assign pod to node
  2. Pull: pull image
  3. Start: start pod
- kube-proxy
  1. manage iptables rules by Service def
  2. Internal DNS & load balancer
- container runtime
  - CRI-O
  - containerd (cri-dockerd)
  - docker (dockershim)
  - mirantis Container Runtime
- cri-dockerd


# K8 Networking
> Container Network Interface (CNI) default Calico-node, not install

> The container runtime offloads the IP assignment to CNI

> IP-per-Pod is each Pod receiving a unique IP address

> container within same pod connect through localhost

> Services, complex encapsulations of network routing rule definitions stored in iptables on cluster nodes and implemented by kube-proxy agents. 

> containerD and cri-o are container runtime

> application layer encryption for secrets

> service mesh

#### VM Driver
- podman
- virtualbox

Normal Users manage by independent services like User/Client Certificates
Service Accounts communicate with the API server to perform various operations.

`sudo cp /etc/kubernetes/manifests/kube-apiserver.yamml /kube-apiserver-backup`
just add extra param
takes 20-40 seconds for api-server restart


The user/client now connects to a Service via its ClusterIP, which forwards traffic to one of the Pods attached to it. A Service provides load balancing by default while selecting the Pods for traffic forwarding.

Service.targetPort = Pod.spec.containerPort

Service's endpoints is auto created by k8, Ex:(10.1.1.1:5000, 10.1.1.2:5000)
can I manually test Service's endpoints? 10.1.1.1:5000

Traffic police
Local: exposed to only same node

my-svc.my-namespace.svc.cluster.local


## Volume Type
> There are so many Volumes, always go by offical docu
> https://kubernetes.io/docs/concepts/storage/volumes/
> https://kubernetes.io/docs/concepts/storage/persistent-volumes/

> Dynamic Storage Provisioning
> https://kubernetes.io/docs/concepts/storage/storage-classes/
- emptyDir `deleted after POD dead`
- hostPath `exists after POD dead. on node?`
- cephfs
- nfs
- iscsi
- secret
  - secret data `must be base64`
  - stringData `is string`
- configMap
- persistentVolumeClaim(PVC)

> StorageClass is similar to StorageProfile


### Cloud volumns
- gcePersistentDisk
- awsElasticBlockStore
- azureDisk
- azureFile


## Job
- parallelism `to set the number of pods allowed to run in parallel;`
- completions `to set the number of expected completions;`
- activeDeadlineSeconds `to set the duration of the Job;`
- backoffLimit `to set the number of retries before Job is marked as failed;`
- ttlSecondsAfterFinished `to delay the clean up of the finished Jobs.`

## CronJobs
- startingDeadlineSeconds `to set the deadline to start a Job if scheduled time was missed;`
- concurrencyPolicy `to allow or forbid concurrent Jobs or to replace old Jobs with new ones. `

## POD
- Quality of Service ['Guaranteed', 'Burstable', 'BestEffort']
- PriorityClass 
- naked Pods `Kind: POD deployment`
- Static Pods `PODs part of k8s system, /etc/kubernetes/manifests/`

### Certificate
- LFS158x
- Kubernetes Fundamentals (LFS258)
- Certified Kubernetes Administrator (CKA)
- Kubernetes for Developers (LFD259)
- Certified Kubernetes Application Developer (CKAD)

