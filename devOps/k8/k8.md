# Kubernetes
> start from Google Borg, since 2014, under Cloud Native Computing Foundation (CNCF)
> 
> Infracture as code
>
> (K8 package)[https://artifacthub.io/]
> 
> k8 needs an `external loadbalancer`, or use MetaLB

## Things Cloud Providers make easier
- IAM `cloud provider already have buildin IAM w nice interface, self host k8 needs self manage auth by cert or service_account_token`
- External Loadbalancer for K8 `cloud have custom loadbalancer assign public ip, alt is MetalLB`
- Logging `all clouds have buildin logging, so many logs alternative but more work to setup, Kibana ELK?`
- Fancy UI way better than any extension


## Analogy
> **External Loadbalancer** (Public Street Entry)
> 
> **Cluster** (Shopping Mall) `Shopping Mall without parking lot, customers needs to walk into store`
> 
> **Node** (Building) `Shopping Mall can have only 1 building, or multi building connected together`
>
> **Pods** (Area inside Shopping Mall Building) `can be [store/app deployment, hallway/loadbalancer, service area/job], Mall mgr able to scale multiple, or move store between building; Area by default is boarded up, to access must create door)`
> 
> **Api Server** (Mall mgr)
>> **etcd** Mall mgr's notebook
>> **Controller, Scheduler** (Assistant of Mall mgr)
>> **kubelet** (Mall Area mgr)
>> **kubeproxy** (Direction Sign of Mall)
> 
> **LoadBalancer Service** (Customer Entry Point to Shopping Mall `able to handle more traffic, in cost of expensive construction`)
>> **Ingress Controller** (Main Entry to Hallway) `route customers to stores(ClusterIP Service)`
>>> **Ingress Rule** (Hallway inside Shopping Mall route customers to Stores `layer 7 Application & layer 4 Transport`)
>
>> **LoadBalancer Service** (Coner Store Door) `Coner Store(Ex: public UI, API) expects a lot traffic will needs its own door(public access)`
> 
> **NodePort Service** (Service Door of Store in Shopping Mall `allow developer testing, usually not use in production`)
> 
> **ClusterIP Service** (Store Door inside Shopping Mall, have smaller limited traffic support)


# Workflow:
1. Authentication
2. Authorization
3. Mutating webhook
4. AdmissionReview
5. Validating webhook
6. Lock etcd
7. Edit etcd
8. api_service watch trigger kube-scheduler 
9. kube-scheduler assign new Pod on some Node w enough resources
10. kubelet send Container Runtime Interface (CRI) request
11. Container Runtime Interface (CRI) calls Container Network Interface(CNI), assign IP for pod
12. Container Network Interface(CNI) call Container Runtime create POD

## Objects / Resources / Manifests
> resources types: https://kubernetes.io/docs/reference/kubectl/overview/#resource-types
> custom resource definition (CRD) since k8 1.7
>
> annotations is NOT used by selector, selector = label
>
> apiversions value dependenced on Kind, check here:
>
> https://matthewpalmer.net/kubernetes-app-developer/articles/kubernetes-apiversion-definition-guide.html
>
> https://kubernetes.io/docs/concepts/overview/working-with-objects/

> Examples of controllers are Deployments, ReplicaSets, DaemonSets, Jobs, etc.
>
> DaemonSets enforce single pod per node on all nodes (for monitor logs, storage, network)
>
> ReplicaSet is outdated
- config `store contexts, where cluster, who is user, ca,`
- job
- cronjob `https://crontab.guru/`
- deployment `Add deployment_strategy, Pros,... on top ReplicaSet, `
- ReplicaSet (RS) `a very basic unit k8 start pods`
- pods `smalleset k8 unit`
- service `exposed to other k8 services, append records into service registry`
- ingress `exposed to vpn`
- event `api_service only store 1hr events`
- secrets
- configMap

## K8 CMDs
```bash
kubectl create job --from=cronjob/mycronjob name-of-one-off-job
kubectl run XXXX --images=XXXX --port=80  
kubectl exec -it pod-name sh
kubectl rollout restart deployment xyz
kubectl rollout undo deployment myapp
kubectl rollout history deploy myapp --revision=2

# Almost never do this in Production, will create service & ingress
kubectl expose deployment XXXX --type="LoadBalancer" service "XXXX" exposed  

# Show all process
ps auxww 
```
## Ingress
> ingress is K8 route controller, by default is nginx, alt is nginx+(Dashboard, LB methods, sess persistance, health check, JWT validation), or traefik, HAProxy https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/

### Ingress Types:
- Singel Service
- Simple Fanout
- Name-based virtual hosting
  
## K8 components:
- Mater Node
  - Scheduler
    - Trigger Cron Jobs
  - Control Mgr
    - Compare Active State vs Desired State
  - etcd
    - key/value DB
  - API service
    - Supports cmds
- Worker Node
  - Container Run Time
    - Pods //smallest k8 unit
  - Kubproxy
    - route & selector
  - Kubelet
    - communicate with API-service

## Node Components
- kubelet
  1. Scheduled: assign pod to node
  2. Pull: pull image
  3. Start: start pod
- kube-proxy
  1. manage iptables rules by Service def
  2. Internal DNS & load balancer
- container runtime
  - CRI-O
  - containerd (cri-dockerd)
  - docker (dockershim)
  - mirantis Container Runtime
- cri-dockerd


# K8 Networking
> Container Network Interface (CNI) default Calico-node

> The container runtime offloads the IP assignment to CNI

> IP-per-Pod is each Pod receiving a unique IP address

> container within same pod connect through localhost

> Services, complex encapsulations of network routing rule definitions stored in iptables on cluster nodes and implemented by kube-proxy agents. 

> containerD and cri-o are container runtime

> application layer encryption for secrets

> service mesh

#### VM Driver
- podman
- virtualbox

Normal Users manage by independent services like User/Client Certificates
Service Accounts communicate with the API server to perform various operations.

`sudo cp /etc/kubernetes/manifests/kube-apiserver.yamml /kube-apiserver-backup`
just add extra param
takes 20-40 seconds for api-server restart


The user/client now connects to a Service via its ClusterIP, which forwards traffic to one of the Pods attached to it. A Service provides load balancing by default while selecting the Pods for traffic forwarding.

Service.targetPort = Pod.spec.containerPort

Service's endpoints is auto created by k8, Ex:(10.1.1.1:5000, 10.1.1.2:5000)
can I manually test Service's endpoints? 10.1.1.1:5000

Traffic police
Local: exposed to only same node

my-svc.my-namespace.svc.cluster.local


## Volume Type
- emptyDir // deleted after POD dead
- hostPath // exists after POD dead. on node?
- cephfs
- nfs
- iscsi
- secret
  - secret data `must be base64`
  - stringData `is string`
- configMap
- persistentVolumeClaim(PVC)


### Cloud volumns
- gcePersistentDisk // GCP
- awsElasticBlockStore
- azureDisk
- azureFile


## Job
- parallelism `to set the number of pods allowed to run in parallel;`
- completions `to set the number of expected completions;`
- activeDeadlineSeconds `to set the duration of the Job;`
- backoffLimit `to set the number of retries before Job is marked as failed;`
- ttlSecondsAfterFinished `to delay the clean up of the finished Jobs.`

## CronJobs
- startingDeadlineSeconds `to set the deadline to start a Job if scheduled time was missed;`
- concurrencyPolicy `to allow or forbid concurrent Jobs or to replace old Jobs with new ones. `

## POD
- Quality of Service ['Guaranteed', 'Burstable', 'BestEffort']
- PriorityClass 

### Certificate
- LFS158x
- Kubernetes Fundamentals (LFS258)
- Certified Kubernetes Administrator (CKA)
- Kubernetes for Developers (LFD259)
- Certified Kubernetes Application Developer (CKAD)

