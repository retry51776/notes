# kubernetes
Helm is package manager for k8, also templating Engines

https://artifacthub.io/

Infracture as code

## Setup gcloud docker
```
gcloud init
gcloud auth login
gcloud config set project XXX
gcloud components update
gcloud components install docker-credential-gcr  
gcloud auth configure-docker

// Test by pulling gcp image & container
gcloud docker -- ls
docker pull [gcr.io/XXX/YOUR_PROJECT_NAME:develop](http://gcr.io/XXX/YOUR_PROJECT_NAME:develop)

source ~/.bashrc 

```

## Setup gcloud kubectl
```
gcloud container clusters get-credentials XXX-cluster --zone us-central1-a
```

## Setup PowerShell Profile shows GCP context
```
# Run profile in powershell to see profile location
$profile

# Make sure profile is accessable
Unblock-File -Path $profile

# Add code to profile
function Prompt {
  $currentContext = kubectl config current-context
  Write-Host ("`r`n[$PWD]($currentContext)")
  return ">";
}
```

`kubectl [create|edit|get|delete]`
- job
- cronjob
- deployment
- pods
- service
- ingress
- event
- secrets


all k8 resources types: https://kubernetes.io/docs/reference/kubectl/overview/#resource-types
https://crontab.guru/

Commons K8 CMD
```

kubectl create job --from=cronjob/mycronjob name-of-one-off-job
kubectl run XXXX --images=XXXX --port=80  
kubectl exec -it pod-name sh
kubectl rollout restart deployment xyz
kubectl rollout undo deployment myapp
kubectl rollout history deploy myapp --revision=2

  
kubectl expose deployment XXXX --type="LoadBalancer" service "XXXX" exposed  
  
ps auxww // Show all process
```



# Tech terms
containerD and cri-o are container runtime
open container initiative


ingress is K8 build in route control traefik similar to ingress, but more features nginx

  
application layer encryption  for secrets

  
components:
- Mater Node
  - Scheduler
    - Trigger Cron Jobs
  - Control Mgr
    - Compare Active State vs Desired State
  - etcd
    - key/value DB
  - API service
    - Supports cmds
- Worker Node
  - Container Run Time
    - Pods //smallest k8 unit
  - Kubproxy
    - route & selector
  - Kubelet
    - communicate with API-service


**Deployment File**

https://kubernetes.io/docs/concepts/overview/working-with-objects/

```
apiVersion: v1 # Required
kind: [Service|other k8 resources] # Required
metadata: # Required: uniquely identify the object
	name: # Required: k8 object id
	labels: # Optional
		app: # Important: must match to selector
		type:
spec: # Required: desire state

# layer 7 tcp load balancer
# redirect traffic to different services
# redirect different ip to matching service (nodePort doesn't matter anymore)

Delete Old Ingress, then recreate ingress. Often apply -f won't overwrite
Or try kubectl replace --force -f xxx.yaml
Ingress type: [Custom | External LB | Internal LB]
Ingress spec:
  rules:
  - host: stage.local
    http:
      paths:
      - path: /
        backend:
          serviceName: stage
          servicePort: web
  - host: terry.internal-service.local
    http:
      paths:
      - path: /app1
        backend:
          serviceName: terry-app1
          servicePort: web
      - path: /app2
        backend:
          serviceName: terry-app2
          servicePort: web
  tls:
  - secretName: terry.internal-service.local

Service spec:
	type: [NodePort|ClusterIP|LoadBalancer] # usually ClusterIP
	ports:
	# ClusterIP: accessable within k8 network
		- port: 80
		- targetPort: 80
		- protocol: TCP
	# NodePort: each pod is accessable by public_ip:port
		- targetPort: 80
		- port: 80
		- nodePort: [30000-32767]
  # LoadBalancer: publish service by single public ip
	selector:
		app: # Important: Match from metadata.labels
		type: # Important: Match from metadata.labels

# Test by `curl https://node_ip:nodePort/`
```

service mesh
see hoe sidecar is srtup in episode manager
hashcorp vault generate dynamic credentials
morpheus gui for none dev people to scale project
operational maturity, sla, slos, slis,
runbook,

grafana conference

Grafana components
Prometheus is time serious database, 
Loki is store logs(same as elastic search)
PromQl is search tool, similar kibana

OpenTelemetry standard for trace front end.
Tempo; trace backend storage; 100% trace storage


*Identity Access Management*
Who? Do what? On What?
Identity
- Google Account
- Service Account
- Group
- Cloud ID

Police -> roles -> premission [service. resource. verb]

Cloud Native Computing Foundation (CNCF)
start from Google Borg

each node has 
**kubelet**
1. Scheduled: assign pod to node
2. Pull: pull image
3. Start: start pod
**kube-proxy**
1. manage iptables rules by Service def
2. Internal DNS & load balancer
**container runtime**
CRI-O
containerd (cri-dockerd)
docker (dockershim)
mirantis Container Runtime

cri-dockerd
Container_Runtime <- Container Runtime Interface <- kubelet <- API_Service


# K8 Networking
Container Network Interface (CNI) default Calico-node
The container runtime offloads the IP assignment to CNI

IP-per-Pod is each Pod receiving a unique IP address
container within same pod connect through localhost

Services, complex encapsulations of network routing rule definitions stored in iptables on cluster nodes and implemented by kube-proxy agents. 

install tools
kubeadm
kubespray/kargo (base off Ansible)
kops

Examples of controllers are Deployments, ReplicaSets, DaemonSets, Jobs, etc.
DaemonSets enforce single pod per node on all nodes (for monitor logs, storage, network)
ReplicaSet is outdated

VM Driver
- podman
- virtualbox

kubectl config view
Default Paths
/var/lib/minikube/certs/
~/.kube/config

Normal Users manage by independent services like User/Client Certificates
Service Accounts communicate with the API server to perform various operations.

authentication modules:
- X509 Client Certificates
- Static Token File
- Bootstrap Tokens
- Service Account Tokens
- OpenID Connect Tokens
- Webhook Token Authentication
- Authenticating Proxy

Add K8 User
```
sudo useradd -s /bin/bash bob
sudo passwd bob

openssl genrsa -out bob.key 2048
openssl req -new -key bob.key -out bob.csr -subj "/CN=bob/O=learner"
// Convert csr to base64
cat bob.csr | base64 | tr -d '\n','%'

vim signing-request.yaml
below is asking k8 premission for certificate we just created
---
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: bob-csr
spec:
  groups:
  - system:authenticated
  request: <assign encoded value from next cat command>
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - digital signature
  - key encipherment
  - client auth

---
// Aprovel Signning Request
kubectl get csr
certificatesigningrequest.certificates.k8s.io/bob-csr approved

// Export Certificate
kubectl get csr bob-csr -o jsonpath='{.status.certificate}' | base64 -d > bob.crt

// Assign bob's credential to key & cert
kubectl config set-credentials bob --client-certificate=bob.crt --client-key=bob.key

// Create context just for bob
kubectl config set-context bob-context --cluster=minikube --namespace=lfs158 --user=bob

//kubectl config view
---
// Give bob role access
vim role.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-reader
  namespace: lfs158
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]

~/rbac$ kubectl create -f role.yaml
```


sudo cp /etc/kubernetes/manifests/kube-apiserver.yamml /kube-apiserver-backup
just add extra param
takes 20-40 seconds for api-server restart


The user/client now connects to a Service via its ClusterIP, which forwards traffic to one of the Pods attached to it. A Service provides load balancing by default while selecting the Pods for traffic forwarding.

Service.targetPort = Pod.spec.containerPort

Service's endpoints is auto created by k8, Ex:(10.1.1.1:5000, 10.1.1.2:5000)
can I manually test Service's endpoints? 10.1.1.1:5000

Traffic police
Local: exposed to only same node

my-svc.my-namespace.svc.cluster.local

to test ClustIP
minikube ssh | gcloud compute ssh <NODE_NAME> --zone <ZONE>
//endpoint is POD's IP
curl endpoint:port
// Each Service has kubeproxy virtual IP
curl service_ip

Volume Type
- emptyDir // deleted after POD dead
- hostPath // exists after POD dead. on node?
- cephfs
- nfs
- iscsi
- secret
- configMap
- persistentVolumeClaim(PVC)


// Cloud
- gcePersistentDisk // GCP
- awsElasticBlockStore
- azureDisk
- azureFile


```
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: blue-app
  name: blue-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: blue-app
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: blue-app
        type: canary
    spec:
      volumes:
      - name: host-volume # Where host folder
        hostPath:
          path: /home/docker/blue-shared-volume
      containers:
      - image: nginx
        name: nginx
        ports:
        - containerPort: 80
        volumeMounts:
        - mountPath: /usr/share/nginx/html # Where folder inside container
          name: host-volume # Which host folder
      - image: debian
        name: debian
        volumeMounts:
        - mountPath: /host-vol # Where folder inside container
          name: host-volume
        command: ["/bin/sh", "-c", "echo Welcome to BLUE App! > /host-vol/index.html ; sleep infinity"]
```

```
  containers:
  - name: myapp-vol-container
    image: myapp
    volumeMounts:
    - name: config-volume
      mountPath: /etc/config
  volumes:
  - name: config-volume
    configMap:
      name: vol-config-map

```

secret data must be base64
stringData is string

annotations is NOT used by selector

Job
- parallelism - `to set the number of pods allowed to run in parallel;`
- completions - `to set the number of expected completions;`
- activeDeadlineSeconds - `to set the duration of the Job;`
- backoffLimit - `to set the number of retries before Job is marked as failed;`
- ttlSecondsAfterFinished - `to delay the clean up of the finished Jobs.`

CronJobs
- startingDeadlineSeconds - `to set the deadline to start a Job if scheduled time was missed;`
- concurrencyPolicy - `to allow or forbid concurrent Jobs or to replace old Jobs with new ones. `

Service Mesh Addons:
- Consul
- Envoy
- Istio
- Kuma
- Linkerd
- Maesh
- Tanzu Service Mesh

Advance Deployement
Canary - 2 different version at same time
Blue/Green - Completed switch over
> Create another service with selector able select both version PODs

**Certificate**
- LFS158x
- Kubernetes Fundamentals (LFS258)
- Certified Kubernetes Administrator (CKA)
- Kubernetes for Developers (LFD259)
- Certified Kubernetes Application Developer (CKAD)


Commons Erros
- Error Validating data
- SchedulingDisabled
- ImagePullBackoff
- CashLoopBackoff
- CreateContainerError
- Probe Failling
- RunContainerError
- Exceed CPU Limits
- FailedScheduling
- Failed Mount
- Kubelet crash

#Helm
helm install --values=xxx.yaml chart_name
Helm < 2.0
Client / Sever (Tiller)
>= 3.0 No more Tiller


`helm template Release1 helm-charts`

/helm-charts
  /charts
  /templates (what k8 objects creates)
  .values.yaml (k8 values)
  .Chart.yaml (chart meta info)

{{ include "helm-cahrts.labels" . | nindent 4 }}
{{ .Values.service.name | qoute }}

// - remove line space
test:
  {{- .Release.Name }}

{{- toYaml .Values.xxx | nindent 8 }}

# Terrform vs Ansible
