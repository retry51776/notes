<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Transformer Explain ‚Äì Generated by GPT-5.1, narrowed by Terry Wu</title>
  <link
    rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css"
    referrerpolicy="no-referrer"
  />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root {
      --bg: #020617;
      --bg-alt: #020617;
      --card: #020617;
      --fg: #e5e7eb;
      --muted: #9ca3af;
      --accent: #38bdf8;
      --accent-soft: rgba(56, 189, 248, 0.08);
      --border: #1f2933;
      --code-bg: #020617;
      --shadow: 0 24px 80px rgba(15, 23, 42, 0.9);
      --radius-xl: 18px;
    }

    * {
      box-sizing: border-box;
    }

    html, body {
      margin: 0;
      padding: 0;
      height: 100%;
      scroll-behavior: smooth;
    }

    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "SF Pro Text",
        "Segoe UI", sans-serif;
      background: radial-gradient(circle at top, #0b1120 0, #020617 55%, #000 100%);
      color: var(--fg);
      overflow-y: auto;
      scroll-snap-type: y mandatory;
    }

    .slides {
      position: relative;
    }

    .slide {
      min-height: 100vh;
      padding: 3.5rem clamp(1.5rem, 6vw, 5rem);
      display: flex;
      flex-direction: column;
      justify-content: center;
      scroll-snap-align: start;
    }

    .card {
      background: radial-gradient(circle at top left, #020617, #020617 40%, #020617 100%);
      border-radius: var(--radius-xl);
      border: 1px solid var(--border);
      box-shadow: var(--shadow);
      padding: clamp(1.75rem, 3vw, 2.5rem);
      max-width: 70rem;
      margin: 0 auto;
      backdrop-filter: blur(18px);
    }

    h1, h2, h3 {
      margin: 0 0 0.75rem;
      letter-spacing: 0.02em;
    }

    h1 {
      font-size: clamp(2.4rem, 4vw, 3.2rem);
    }

    h2 {
      font-size: clamp(1.8rem, 3vw, 2.3rem);
    }

    h3 {
      font-size: clamp(1.3rem, 2.2vw, 1.6rem);
      color: var(--muted);
      font-weight: 500;
    }

    p {
      margin: 0.25rem 0 0.75rem;
      color: var(--muted);
      max-width: 60rem;
    }

    .hero-blurb {
      font-size: 1.05rem;
      color: #cbd5f5;
      margin: 0.4rem 0 1rem;
      max-width: 52rem;
    }

    ul {
      margin: 0.5rem 0 0;
      padding-left: 0;
      list-style: none;
    }

    li {
      margin: 0.25rem 0;
      line-height: 1.55;
      position: relative;
      padding-left: 1.4rem;
    }

    li::before {
      content: "‚ú¶";
      position: absolute;
      left: 0;
      top: 0.2rem;
      color: var(--accent);
      font-size: 0.8rem;
    }

    li ul {
      margin-top: 0.35rem;
      margin-left: 0.6rem;
    }

    li > strong:first-child {
      display: inline-flex;
      align-items: center;
      gap: 0.35rem;
    }

    .pill-row {
      display: flex;
      flex-wrap: wrap;
      gap: 0.6rem;
      margin: 0.75rem 0 0.5rem;
    }

    .pill {
      border-radius: 999px;
      border: 1px solid rgba(148, 163, 184, 0.4);
      padding: 0.2rem 0.7rem;
      font-size: 0.78rem;
      text-transform: uppercase;
      letter-spacing: 0.09em;
      color: var(--muted);
      background: linear-gradient(120deg, rgba(15, 23, 42, 0.9), rgba(15, 23, 42, 0.5));
      display: inline-flex;
      align-items: center;
      gap: 0.35rem;
    }

    .pill-dot {
      width: 7px;
      height: 7px;
      border-radius: 999px;
      background: var(--accent);
      box-shadow: 0 0 0 4px rgba(56, 189, 248, 0.25);
    }

    .flow {
      margin: 0.75rem 0;
      padding: 0.9rem 1rem;
      border-radius: 12px;
      background: radial-gradient(circle at left, #0b1120, #020617);
      border: 1px solid rgba(148, 163, 184, 0.4);
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas,
        "Liberation Mono", "Courier New", monospace;
      font-size: 0.86rem;
      overflow-x: auto;
      white-space: nowrap;
    }

    .flow-annotated {
      display: flex;
      flex-direction: column;
      gap: 0.2rem;
      white-space: normal;
    }

    .flow-layer {
      font-family: "Space Grotesk", "Segoe UI", system-ui, -apple-system,
        BlinkMacSystemFont, sans-serif;
      font-size: 0.68rem;
      text-transform: uppercase;
      letter-spacing: 0.2em;
      color: var(--muted);
      display: inline-flex;
      align-items: center;
      gap: 0.35rem;
      margin-top: 0.4rem;
    }

    .flow-layer::before {
      content: "";
      width: 12px;
      height: 2px;
      border-radius: 999px;
      background: rgba(148, 163, 184, 0.6);
      display: inline-block;
    }

    .flow-layer-algo::before {
      background: rgba(56, 189, 248, 0.7);
    }

    .flow-layer-core {
      color: #facc15;
    }

    .flow-layer-core::before {
      background: rgba(250, 204, 21, 0.7);
    }

    .flow-step {
      display: block;
      line-height: 1.5;
    }

    .flow-step-algo {
      color: cadetblue;
    }

    .flow-step-core {
      font-family: "Space Grotesk", "Segoe UI", system-ui, -apple-system,
        BlinkMacSystemFont, sans-serif;
      font-size: 0.92rem;
      color: #facc15;
      background: rgba(250, 204, 21, 0.08);
      border: 1px solid rgba(250, 204, 21, 0.4);
      padding: 0.35rem 0.6rem;
      border-radius: 10px;
      box-shadow: 0 0 0 1px rgba(250, 204, 21, 0.15);
    }

    code, pre {
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas,
        "Liberation Mono", "Courier New", monospace;
      font-size: 0.86rem;
    }

    pre {
      margin: 0.6rem 0;
      padding: 0.8rem 1rem;
      border-radius: 12px;
      background: var(--code-bg);
      border: 1px solid rgba(30, 64, 175, 0.8);
      overflow-x: auto;
    }

    .two-col {
      display: grid;
      grid-template-columns: minmax(0, 1.2fr) minmax(0, 1fr);
      gap: 1.5rem;
      margin-top: 0.75rem;
    }

    .two-col-50 {
      display: grid;
      grid-template-columns: repeat(2, minmax(0, 1fr));
      gap: 1.5rem;
      margin-top: 0.75rem;
    }

    .col-box {
      border-radius: 14px;
      border: 1px solid rgba(148, 163, 184, 0.4);
      padding: 0.8rem 1rem;
      background: radial-gradient(circle at top, #020617, #020617);
    }

    .col-title {
      font-size: 0.9rem;
      text-transform: uppercase;
      letter-spacing: 0.09em;
      color: var(--muted);
      margin-bottom: 0.35rem;
    }

    .tag {
      display: inline-flex;
      align-items: center;
      gap: 0.35rem;
      padding: 0.14rem 0.55rem;
      border-radius: 999px;
      background: rgba(56, 189, 248, 0.08);
      border: 1px solid rgba(56, 189, 248, 0.5);
      font-size: 0.72rem;
      text-transform: uppercase;
      letter-spacing: 0.12em;
      color: var(--accent);
      margin-right: 0.4rem;
    }

    .tag-dot {
      width: 6px;
      height: 6px;
      border-radius: 999px;
      background: var(--accent);
    }

    .badge-row {
      display: flex;
      flex-wrap: wrap;
      gap: 0.4rem;
      margin-top: 0.45rem;
      font-size: 0.75rem;
    }

    .badge {
      padding: 0.18rem 0.55rem;
      border-radius: 999px;
      border: 1px solid rgba(148, 163, 184, 0.4);
      background: rgba(15, 23, 42, 0.9);
      color: var(--muted);
    }

    .visual-frame {
      margin-top: 1.1rem;
      padding: 1rem;
      border-radius: 18px;
      border: 1px solid rgba(148, 163, 184, 0.35);
      background: radial-gradient(circle at top, rgba(15, 23, 42, 0.85), rgba(2, 6, 23, 0.95));
      box-shadow: 0 30px 60px rgba(2, 6, 23, 0.8);
    }

    .visual-frame img {
      width: 100%;
      height: auto;
      display: block;
      border-radius: 12px;
    }

    .nav-hint {
      position: fixed;
      right: 1.4rem;
      bottom: 1.1rem;
      font-size: 0.78rem;
      color: rgba(156, 163, 175, 0.7);
      background: rgba(15, 23, 42, 0.9);
      padding: 0.4rem 0.7rem;
      border-radius: 999px;
      border: 1px solid rgba(55, 65, 81, 0.8);
      backdrop-filter: blur(12px);
      z-index: 20;
    }

    .slide-num {
      position: fixed;
      left: 1.5rem;
      top: 1rem;
      font-size: 0.78rem;
      color: rgba(148, 163, 184, 0.85);
      text-transform: uppercase;
      letter-spacing: 0.18em;
      z-index: 20;
    }

    a {
      color: var(--accent);
      text-decoration: none;
      border-bottom: 1px dotted rgba(56, 189, 248, 0.6);
    }

    a:hover {
      text-decoration: underline;
      text-underline-offset: 2px;
    }

    .emoji {
      font-size: 1.05rem;
    }

    @media (max-width: 768px) {
      .slide {
        padding-inline: 1.2rem;
      }
      .two-col,
      .two-col-50 {
        grid-template-columns: minmax(0, 1fr);
      }
      .card {
        padding-inline: 1.3rem;
      }
    }
  </style>
</head>
<body>
  <div class="slide-num">TRANSFORMER WORKFLOW ¬∑ TECHNICAL</div>
  <div class="nav-hint">Scroll ‚Üì for next slide</div>

  <main class="slides">
    <!-- Slide 1 ‚Äì Title -->
    <section class="slide" id="slide-1">
      <div class="card">
        <div class="pill-row">
          <div class="pill">
            <span class="pill-dot"></span>
            Transformer Explain
          </div>
        </div>
        <h1>Transformer Explain</h1>
        <h3>Generated by GPT-5.1 ¬∑ narrowed by Terry Wu</h3>
        <p class="hero-blurb">
          Let's open the hood of Transformer LLMs.
        </p>
        <div class="badge-row">
          <div class="badge"><span class="emoji">üß†</span> Developer centric</div>
          <div class="badge"><span class="emoji">üìà</span> Compute + architecture</div>
          <div class="badge"><span class="emoji">üß™</span> No mathematics</div>
        </div>
      </div>
    </section>

    <!-- Slide 2 ‚Äì Outline -->
    <section class="slide" id="slide-2">
      <div class="card">
        <span class="tag"><span class="tag-dot"></span>Orientation</span>
        <h2>Outline</h2>
        <ul>
          <li><strong>Goal</strong>: follow the <strong>technical dataflow</strong> inside a GPT-style Transformer from prompt text to the next token.</li>
          <li><strong>Focus</strong>:
            <ul>
              <li>Token pipeline, residual stream, GPT-OSS chat template, and special tokens.</li>
              <li>Batching, KV cache, MoE routing, prefill vs. decode timing, RL acting on logits.</li>
            </ul>
          </li>
        </ul>
      </div>
    </section>

    <!-- Slide 3 ‚Äì Visual Teaser -->
    <section class="slide" id="slide-2a">
      <div class="card">
        <span class="tag"><span class="tag-dot"></span>Visual Cue</span>
        <h2>What We‚Äôre Explaining</h2>
        <p>Quick situational awareness for the system diagram we will unpack in the next slides.</p>
        <div class="visual-frame">
          <img src="teaser.png" alt="Transformer workflow teaser diagram" loading="lazy" />
        </div>
      </div>
    </section>

    <!-- Slide 3 ‚Äì High-level pipeline -->
    <section class="slide" id="slide-3">
      <div class="card">
        <span class="tag"><span class="tag-dot"></span>Pipeline Overview</span>
        <h2>End-to-End Workflow</h2>
        <p>We work with a single forward generation step on top of an existing prompt.</p>

        <div class="flow flow-annotated">
          <div class="flow-layer flow-layer-algo">Algorithmic Layer ¬∑ <a href="https://github.com/karpathy/nanochat/blob/4a87a0d19f30799b6c700285822dcca850adf6a4/scripts/chat_cli.py#L76">Prompt Handling</a></div>
          <div class="flow-step flow-step-algo">‚òÖ prompt: List[str]</div>
          <div class="flow-step flow-step-algo">‚Üì template: List[str]</div>
          <div class="flow-step flow-step-algo">‚Üì tokens: List[int]</div>
          <div class="flow-step flow-step-algo">‚Üì embeddings: List[matrix[double]] (shape ‚âà [seq_len, d_model])</div>
          <div class="flow-layer flow-layer-core"><a href="https://github.com/karpathy/nanochat/blob/4a87a0d19f30799b6c700285822dcca850adf6a4/nanochat/engine.py#L218">Computation Core</a></div>
          <div class="flow-step flow-step-core">‚Üì residual_stream: List[matrix[double]] (black magic happens here)</div>
          <div class="flow-layer flow-layer-algo">Algorithmic Layer ¬∑ <a href="https://github.com/karpathy/nanochat/blob/4a87a0d19f30799b6c700285822dcca850adf6a4/scripts/chat_cli.py#L93">Decode Loop</a></div>
          <div class="flow-step flow-step-algo">‚Üì new_token_logit: matrix[double][-1] (shape ‚âà [vocab_size])</div>
          <div class="flow-step flow-step-algo">‚Üì greedy_decode: argMax(new_token_logit)</div>
          <div class="flow-step flow-step-algo">‚Üì token: int</div>
          <div class="flow-step flow-step-algo">‚Üì new_word: str</div>
          <div class="flow-step flow-step-algo">‚Üª append to prompt &amp; repeat (autoregressive)</div>
        </div>

        <ul>
          <li><strong>Autoregressive</strong>: each step predicts exactly one new token conditioned on all previous tokens.</li>
          <li><strong>Residual stream</strong>: shared vector at each position that all attention/MLP blocks read &amp; write.</li>
          <li>Generation loop continues until <code>&lt;|endoftext|&gt;</code> or other stop condition.</li>
        </ul>
      </div>
    </section>


    <!-- Slide 4 ‚Äì Algorithmic vs Computation Core -->
    <section class="slide" id="slide-5">
      <div class="card">
        <span class="tag"><span class="tag-dot"></span>Decomposition</span>
        <h2>LLM = Algorithmic Layer ‚äï Computation Core</h2>

        <div class="two-col-50">
          <div class="col-box">
            <div class="col-title">Algorithmic Layer</div>
            <ul>
              <li>Classic Python code.</li>
              <li>Encoding Logic:
                <ul>
                  <li>Message formatting (e.g. role, channel, tool).</li>
                  <li>Context management (e.g. system message, relevance context).</li>
                  <li>LLM Controlled variables (e.g. thinking_effort).</li>
                </ul>
              </li>
              <li>Decoding strategy (greedy, top-k, draft tokens).</li>
              <li>Batching policy &amp; its complications.</li>
              <li>KV-cache management &amp; scheduling.</li>
            </ul>
          </div>
          <div class="col-box">
            <div class="col-title">Computation Core</div>
            <ul>
              <li><a href="https://github.com/karpathy/nanochat/blob/4a87a0d19f30799b6c700285822dcca850adf6a4/nanochat/gpt.py#L133">Model architecture:</a>
                <ul>
                  <li>
                    <ul>Attention Block
                      <li>Position encoding (ROPE).</li>
                      <li>Attention pattern. </li>
                    </ul>
                  </li>
                  <li>
                    <ul>Feed Forward Block
                      <li>Dense MLP</li>
                      <li>Mixture-of-Experts (MoE) MLP</li>
                    </ul>
                  </li>

              <li>Layer (depth), <code>d_model</code> (width)</li>
                </ul>
              </li>
              <li>
                Training lives here:
                <ul>
                  <li>Pre-training(memorization)</li>
                  <li>Reinforcement Learning (RL) fine-tuning</li>
                </ul>
              </li>
              <li>Mechanistic Interpretability</li>
            </ul>
          </div>
        </div>
      </div>
    </section>

    <!-- Slide 5 ‚Äì Data structures & residual stream -->
    <section class="slide" id="slide-4">
      <div class="card">
        <span class="tag"><span class="tag-dot"></span>Internal Representations</span>
        <h2>Encode vs Decode</h2>

        <div class="two-col">
          <div class="col-box">
            <h3>Encode</h3>
            <div class="col-title">Embeddings(beginning of Residual Stream)</div>
            <ul>
              <li><strong>Prompt</strong>: Python-like <code>List[str]</code> after chat template rendering.</li>
              <li><strong>Tokenizer</strong> maps text to ids using BPE; vocabulary includes reserved tokens like <code>&lt;|startoftext|&gt;</code>, <code>&lt;|endoftext|&gt;</code>, <code>&lt;|endofprompt|&gt;</code>.  <a href="https://huggingface.co/openai/gpt-oss-120b/blob/main/tokenizer_config.json?utm_source=chatgpt.com" target="_blank" rel="noreferrer">tokenizer_config.json</a></li>
              <li><strong>Embedding table</strong>: <code>matrix[double]</code> (conceptually) with shape
                <code>[vocab_size, d_model]</code>. Lookup converts <code>token:int</code> ‚Üí <code>embedding:vector[d_model]</code>.
              </li>
            </ul>
          </div>
          <div class="col-box">
            <h3>Decode</h3>
            <div class="col-title">Residual Stream ‚Üí Logits ‚Üí Token ‚Üí Str</div>
            <ul>
              <li>For each layer ‚Ñì and position t we maintain
                <code>R[‚Ñì, t] ‚àà ‚Ñù<sup>d_model</sup></code> ‚Äì the <strong>residual stream</strong>.
              </li>
              <li>Each block (attention, MoE-MLP) reads <code>R</code>, adds its contribution back into <code>R</code>.</li>
              <li>Final layer projects <code>R[last_layer, last_token]</code> through
                <code>W<sub>lm_head</sub></code> ‚Üí <strong>logits</strong> <code>‚Ñù<sup>vocab_size</sup></code>.
              </li>
              <li>Softmax(logits) ‚Üí probabilities; decoder chooses next token via greedy / sampling.</li>
            </ul>
          </div>
        </div>
      </div>
    </section>

    <!-- Slide 6 ‚Äì GPT-OSS chat template -->
    <section class="slide" id="slide-6">
      <div class="card">
        <span class="tag"><span class="tag-dot"></span>Prompt Formatting</span>
        <h2>GPT-OSS Chat Template (High Level)</h2>
        <p>
          GPT-OSS-120B ships with a <a href="https://huggingface.co/openai/gpt-oss-120b/blob/main/chat_template.jinja?utm_source=chatgpt.com" target="_blank" rel="noreferrer">Jinja2 chat template</a> that converts <code>messages</code> into a single training-style text sequence.
        </p>

        <div class="badge-row">
          <div class="badge"><span class="emoji">üß©</span> roles: <code>system, user, assistant, tool</code></div>
          <div class="badge"><span class="emoji">üì°</span> tool calling support</div>
          <div class="badge"><span class="emoji">üß†</span> reasoning_effort control</div>
        </div>

        <pre><code>{# schematic only, not exact template #}
&lt;|startoftext|&gt;
&lt;|start|&gt;  ‚Üí role
&lt;|channel|&gt;  ‚Üí specifies what type of message (e.g. analysis, final)
&lt;|message|&gt;  ‚Üí starts the content body
&lt;|end|&gt;  ‚Üí closes the whole message

&lt;|start|&gt;
...
&lt;end|&gt;
...
&lt;|endofprompt|&gt;</code></pre>

        <ul>
          <li><strong>role</strong>: determines how each message is tagged in the rendered text.</li>
          <li><strong><code>&lt;|channel|&gt;</code></strong>: special markers used to separate channels like <code>analysis</code>, <code>final</code>, <code>commentary</code> inside the same sequence.</li>
          <li><strong>model_identity</strong>: LLM behavior.  </li>
          <li><strong>reasoning_effort</strong>: hint token/field that asks model for shallow vs deep chain-of-thought (handled in the template kwargs and by training).</li>
          <li><strong>current_date</strong> &amp; <strong>knowledge_cutoff</strong>: injected into the system preamble to anchor model‚Äôs time awareness.</li>
        </ul>
      </div>
    </section>

    <!-- Slide 7 ‚Äì GPT-OSS special tokens -->
    <section class="slide" id="slide-7">
      <div class="card">
        <span class="tag"><span class="tag-dot"></span>Tokenizer</span>
        <h2>GPT-OSS Reserved Tokens (Control Plane)</h2>
        <p>
          GPT-OSS defines reserved special tokens in its tokenizer config, mapped to high-id entries.  <a href="https://huggingface.co/openai/gpt-oss-120b/blob/main/tokenizer_config.json?utm_source=chatgpt.com" target="_blank" rel="noreferrer">oai_citation:4‚Ä°Hugging Face</a>
        </p>

        <div class="two-col-50">
          <div class="col-box">
            <div class="col-title">Key Reserved Tokens</div>
            <ul>
              <li><code>&lt;|startoftext|&gt;</code> ‚Äì sequence start; often combined with system prompt.</li>
              <li><code>&lt;|endoftext|&gt;</code> ‚Äì sequence end / stop condition.</li>
              <li><code>&lt;|endofprompt|&gt;</code> ‚Äì boundary between prompt and model response.</li>
              <li><code>&lt;|call|&gt;</code> ‚Äì marks tool call segments.</li>
              <li><code>&lt;|return|&gt;</code> ‚Äì marks tool return segments.</li>
            </ul>
          </div>
          <div class="col-box">
            <div class="col-title">Why They Matter</div>
            <ul>
              <li>Let one model handle <strong>multiple channels</strong> (thoughts, tools, final answer) in a single flat token sequence.</li>
              <li>Tokenizer config ensures these tokens are <strong>never split</strong> and are treated as atomic IDs.</li>
              <li>Inference engines must honor these tokens when:
                <ul>
                  <li>stripping tool calls,</li>
                  <li>hiding chain-of-thought,</li>
                  <li>or routing to tools.</li>
                </ul>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </section>

    <!-- Slide 8 ‚Äì Algorithmic perf: batching, continuous batching, KV cache -->
    <section class="slide" id="slide-8">
      <div class="card">
        <span class="tag"><span class="tag-dot"></span>Algorithmic Optimizations</span>
        <h2>Performance Levers (Outside the Weights)</h2>

        <div class="two-col">
          <div class="col-box">
            <div class="col-title">1. Batching &amp; Continuous Batching</div>
            <ul>
              <li><strong>Static batching</strong>: group N prompts into a single tensor
                <code>[N, seq_len]</code> for prefill ‚Üí amortize kernel launch / memory traffic.  <a href="    2. https://github.com/ml-explore/mlx-lm/blob/ba2cf3c0ee72e6a8f927009c6ffe8db913decb49/mlx_lm/generate.py#L1088" target="_blank" rel="noreferrer">MLX Batch SDK</a></li>
              <li><strong>Continuous batching</strong>: dynamically add/remove sequences mid-flight so GPUs stay full as requests start/finish.  <a href="https://github.com/vllm-project/vllm/blob/v0.2.7/vllm/core/scheduler.py#L217-L262" target="_blank" rel="noreferrer">vllm continues batching</a></li>

            </ul>
            <pre><code># schematic (not exact MLX code)
for step in steps:
    batch = scheduler.next_ready_requests()
    logits, kv_cache = model(batch.tokens, kv_cache)
    scheduler.feed_logits(batch, logits)</code></pre>
          </div>
          <div class="col-box">
            <div class="col-title">2. KV-Cache (Key/Value Cache)</div>
            <ul>
              <li>During prefill, attention keys &amp; values for each layer and token are stored in the <strong>KV cache</strong>.</li>
              <li>During decode, new tokens attend to <em>cached</em> past tokens instead of recomputing the whole prefix ‚Üí cost per step ~O(context_length) instead of O(context_length¬≤).</li>
              <li>Algorithmic design:
                <ul>
                  <li>cache layout (e.g. paged KV, GPU-friendly strides),</li>
                  <li>lifetime / eviction policy,</li>
                  <li>sharing KV between draft &amp; target models for speculative decoding.</li>
                </ul>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </section>

    <!-- Slide 9 ‚Äì Computation core: GPT-OSS-120B architecture -->
    <section class="slide" id="slide-9">
      <div class="card">
        <span class="tag"><span class="tag-dot"></span>Computation Core</span>
        <h2>GPT-OSS-120B Architecture Snapshot</h2>
        <p>
          GPT-OSS-120B is a <strong>decoder-only Transformer with Mixture-of-Experts (MoE)</strong> layers.
        </p>

        <div class="two-col">
          <div class="col-box">
            <div class="col-title">Key Hyperparameters</div>
            <ul>
              <li><strong>Layers</strong>: 36 Transformer blocks. </li>
              <li><strong>Hidden size</strong> <code>d_model</code>: 2880.</li>
              <li><strong>Attention</strong>:
                <ul>
                  <li>Head dim: 64.</li>
                  <li>Grouped multi-query attention; <code>num_key_value_heads = 8</code>.</li>
                  <li>Alternating dense &amp; locally banded sparse patterns for long context.</li>
                </ul>
              </li>
              <li><strong>Context length</strong>: up to ‚âà128k tokens, depending on runtime.</li>
            </ul>
          </div>
          <div class="col-box">
            <div class="col-title">MoE &amp; Quantization</div>
            <ul>
              <li><strong>Mixture-of-Experts (MoE)</strong>:
                <ul>
                  <li>128 experts per MoE layer, 4 experts active per token.  <a href="https://www.cometapi.com/gpt-oss-120b/?utm_source=chatgpt.com" target="_blank" rel="noreferrer">oai_citation:14‚Ä°CometAPI</a></li>
                  <li>117B total params, ~5.1B active per token forward pass.  <a href="https://openai.com/index/introducing-gpt-oss/?utm_source=chatgpt.com" target="_blank" rel="noreferrer">oai_citation:15‚Ä°OpenAI</a></li>
                </ul>
              </li>
              <li><strong>Routing</strong>: learned router chooses top-k experts per token.</li>
              <li><strong>Quantization</strong>: native MXFP4 config in <code>config.json</code> for efficient FP4 runtimes.  </li>
              <li>All transforms operate on the <strong>residual stream</strong> we saw earlier; algorithmic side decides <em>when</em> to call this giant function.</li>
            </ul>
          </div>
        </div>
      </div>
    </section>

    <!-- Slide 10 ‚Äì Prefill vs Decode (Transformer Explainer) -->
    <section class="slide" id="slide-10">
      <div class="card">
        <span class="tag"><span class="tag-dot"></span>Execution Phases</span>
        <h2>Prefill vs Decode</h2>
        <p>
          The <a href="https://poloclub.github.io/transformer-explainer/" target="_blank" rel="noreferrer">Transformer Explainer</a> shows a live GPT-2 model and visualizes each attention &amp; MLP step.  <a href="https://poloclub.github.io/transformer-explainer/?utm_source=chatgpt.com" target="_blank" rel="noreferrer">oai_citation:18‚Ä°Georgia Tech Data Science Club</a>
        </p>

        <div class="two-col-50">
          <div class="col-box">
            <div class="col-title">Prefill Phase (Parallel, Compute-Intense)</div>
            <ul>
              <li>Input: whole prompt tokens <code>[t‚ÇÅ ‚Ä¶ tL]</code>.</li>
              <li>All positions processed in parallel:
                <ul>
                  <li>calculate KV value for each layer &amp; token</li>
                  <li>Feed forward network</li>
                  <li>MLA compress kv cache</li>
                  <li>all token logit & probs</li>
                </ul>
              </li>
              <li>Perfect place for <strong>large batches</strong> ‚Üí GPUs highly utilized.</li>
              <li>Reason LLMs can be trained at scale: prefill-style passes over huge corpora.</li>
            </ul>
          </div>
          <div class="col-box">
            <div class="col-title">Decode Phase (Serial, RAM-Intense)</div>
            <ul>
              <li>Loop over steps:
                <code>for step in range(max_new_tokens)</code>:
                <ul>
                  <li>feed only latest token,</li>
                  <li>attend over KV cache (prefix),</li>
                  <li>update KV with new token.</li>
                </ul>
              </li>
              <li><strong>Serial dependency</strong>: token t+1 depends on token t ‚Üí less parallelism than prefill.</li>
              <li>Memory bound: KV cache grows as <code>O(L ¬∑ layers ¬∑ d_head)</code>.</li>
            </ul>
          </div>
        </div>
      </div>
    </section>

    <!-- Slide 11 ‚Äì Decoding strategies & speculative decoding -->
    <section class="slide" id="slide-11">
      <div class="card">
        <span class="tag"><span class="tag-dot"></span>Decoding Algorithms</span>
        <h2>Greedy, Sampling, and Speculative Decoding</h2>

        <div class="two-col">
          <div class="col-box">
            <div class="col-title">Greedy vs Top-k / Temperature</div>
            <ul>
              <li><strong>Greedy</strong>: <code>argMax(logits)</code> each step:
                <ul>
                  <li>Deterministic, fastest.</li>
                  <li>Can be repetitive / brittle.</li>
                </ul>
              </li>
              <li><strong>Top-k sampling</strong>:
                <ul>
                  <li>Keep top-k logits, renormalize softmax.</li>
                  <li>Use <strong>temperature</strong> œÑ &gt; 0 to control entropy:
                    <code>softmax(logits / œÑ)</code>.
                  </li>
                </ul>
              </li>
              <li><strong>Top-p (nucleus)</strong>:
                <ul>
                  <li>Minimal token set with cumulative probability ‚â• p.</li>
                  <li>Better adapts to ‚Äúpeaked‚Äù vs ‚Äúflat‚Äù distributions.</li>
                </ul>
              </li>
            </ul>
          </div>

          <div class="col-box">
            <div class="col-title">Speculative Decoding (Draft + Target)</div>
            <ul>
              <li>Use a small <strong>draft model</strong> to propose multiple tokens in parallel.</li>
              <li>Large <strong>target model</strong> validates them in one batched pass:
                <ul>
                  <li>Accept prefix of draft tokens whose probabilities agree.</li>
                  <li>Reject &amp; fall back when they diverge.</li>
                </ul>
              </li>
              <li>Key idea: <strong>trade extra FLOPs for fewer serial steps</strong> in the big model.</li>
              <li>KV-cache sharing between draft/target is an additional optimization layer.</li>
            </ul>
          </div>
        </div>
      </div>
    </section>

    <!-- Slide 12 ‚Äì RL as logit adjustment -->
    <section class="slide" id="slide-12">
      <div class="card">
        <span class="tag"><span class="tag-dot"></span>Training ‚Äì RL View</span>
        <h2>Reinforcement Learning = Adjusting Logits</h2>

        <p>High level: RL modifies the computation core so that the <strong>logits at generation time</strong> line up with some reward signal.</p>

        <div class="two-col-50">
          <div class="col-box">
            <div class="col-title">What RL Touches</div>
            <ul>
              <li>We observe a trajectory (prompt, generated tokens, reward).</li>
              <li>Log probabilities of tokens come from logits:
                <code>log œÄ<sub>Œ∏</sub>(a | s)</code>.
              </li>
              <li>RL update nudges Œ∏ to:
                <ul>
                  <li>increase log-prob of high-reward sequences,</li>
                  <li>decrease log-prob of low-reward ones.</li>
                </ul>
              </li>
              <li>In practice: PPO-style RLHF / RLAIF on top of supervised pre-training.</li>
            </ul>
          </div>
          <div class="col-box">
            <div class="col-title">Diverse &amp; Verified Solutions (Lottery Analogy)</div>
            <ul>
              <li>Think of logits as a <strong>lottery factory</strong>:
                <ul>
                  <li>Factory generates many candidate tickets (token sequences).</li>
                  <li>Reward model / humans ‚Äúvalidate‚Äù which tickets are good.</li>
                </ul>
              </li>
              <li>RL shapes the distribution so that:
                <ul>
                  <li>Good tickets become <strong>more common</strong>,</li>
                  <li>but we keep some entropy for diversity / exploration.</li>
                </ul>
              </li>
              <li>End result: at inference, <strong>greedy or low-temperature sampling</strong> already aligns with human-preferred behavior because the logits moved during RL.</li>
            </ul>
          </div>
        </div>
      </div>
    </section>

    <!-- Slide 13 ‚Äì Wrap -->
    <section class="slide" id="slide-13">
      <div class="card">
        <span class="tag"><span class="tag-dot"></span>Summary</span>
        <h2>Thank You üôè</h2>
        <ul>
          <li>We followed the path: <strong>prompt ‚Üí tokens ‚Üí embeddings ‚Üí residual stream ‚Üí logits ‚Üí decoding</strong>.</li>
          <li>We separated the <strong>Algorithmic layer</strong> (chat template, batching, KV cache, decoding, speculative decode) from the <strong><a href="https://github.com/karpathy/nanochat/blob/4a87a0d19f30799b6c700285822dcca850adf6a4/nanochat/engine.py#L218">Computation core</a></strong> (GPT-OSS-120B MoE architecture &amp; training).</li>
          <li>We connected <strong>RL</strong> directly to <strong>logit shaping</strong>, explaining why ‚Äúdiverse but verified‚Äù generations are possible without magic or crystal balls.</li>
        </ul>
      </div>
    </section>
  </main>
</body>
</html>
